{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XY7I_gYVKEsa"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykim71/georgia_analysis/blob/main/(1)_Georgia_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7PpN2VVxH5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need access to the folder(s) below:\n",
        "\n",
        "#%cd /content/drive/MyDrive/Georgia/\n"
      ],
      "metadata": {
        "id": "aPvEBRBtxzje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data load/prep for translation"
      ],
      "metadata": {
        "id": "yDB2Hw3DPhw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### previous data (for tracking history purpose-only)"
      ],
      "metadata": {
        "id": "rNLrohlw7Wh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not needed anymore\n",
        "#%cd /content/drive/MyDrive/Marquee data\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "path = os.getcwd()\n",
        "files = os.listdir(path)\n",
        "\n",
        "files_xls = [f for f in files if f[-4:] == 'xlsx']\n",
        "files_xls\n"
      ],
      "metadata": {
        "id": "jF_YfqrxyWsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "\n",
        "for f in files_xls:\n",
        "    data = pd.read_excel(f, dtype=object)\n",
        "    df_list.append(data)"
      ],
      "metadata": {
        "id": "GH8Cba7Mytdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_frame = pd.concat(df_list, axis=0, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "eHnmlNhpyt_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_frame_nna = df_frame.dropna(subset=['article_text'])"
      ],
      "metadata": {
        "id": "yHhRdGZq82MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_frame_nna = df_frame_nna.sort_values(by='source')"
      ],
      "metadata": {
        "id": "-yyujE1q1lMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_count = df_frame_nna.groupby('source').count()"
      ],
      "metadata": {
        "id": "N7w2TW1q4-fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(df_frame_nna.date_published)"
      ],
      "metadata": {
        "id": "eERbYQqk5Bz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df_frame_nna.date_published)"
      ],
      "metadata": {
        "id": "edzi8qXk5RSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove line breaks\n",
        "df_frame_nna['article_text'] = df_frame_nna['article_text'].astype(str)\n",
        "df_frame_nna['article_text'] = df_frame_nna['article_text'].map(lambda x: x.replace('\\n', ' '))"
      ],
      "metadata": {
        "id": "B7xHPwlf9ZfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Georgia\n",
        "\n",
        "df_frame_nna.to_pickle('Georgia_data_all.pkl')"
      ],
      "metadata": {
        "id": "RqniIriS9tzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_frame_nna = pd.read_pickle('Georgia_data_all.pkl')"
      ],
      "metadata": {
        "id": "En-I--YQYvG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_frame_nna = pd.read_pickle('Georgia_data_all_ner_lem.pkl')"
      ],
      "metadata": {
        "id": "vwX2pTsHE9Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = df_frame_nna.groupby('source')['article_text'].count()"
      ],
      "metadata": {
        "id": "s4UJHjHuFAQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count.to_excel('Georgia_source_list.xlsx')"
      ],
      "metadata": {
        "id": "WjDri-1SFLc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### updated version"
      ],
      "metadata": {
        "id": "XWekPoyZ7Y5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Georgia (1)/Web Scraping /Updated Marquee data/delivery_2023-03-02.xlsx')"
      ],
      "metadata": {
        "id": "-Pi2-GOT7aqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "_bmQHcM37zF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = df.groupby(['source']).count().reset_index()\n",
        "temp[temp['source'].str.contains(\"Northside Neighbor|Paulding Neighbor|South Fulton Neighbor|Marietta Daily Journal\")]\n",
        "# Northside Neighbor, Paulding Neighbor, South Fulton Neighbor, Marietta Daily Journal"
      ],
      "metadata": {
        "id": "TLeQNgt47iOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nna = df.dropna(subset=['article_text']) # this was for checking to see how many duplicated articles (some sources function like AP publishing the same articles)"
      ],
      "metadata": {
        "id": "GFyjTZO2Ek2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_nna))\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "StT4BWGdFKnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nna_sample = df_nna.groupby('source').apply(lambda x: x.sample(10, replace=False)).reset_index(drop=True) # see random 10 samplesl but there are some sources that have less than 10 articles published\n",
        "df_nna_sample.to_excel('df_nna_sample.xlsx')"
      ],
      "metadata": {
        "id": "RurI0ukfHTjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_nna_sample)"
      ],
      "metadata": {
        "id": "FCo6PRQfIk-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### translation"
      ],
      "metadata": {
        "id": "XY7I_gYVKEsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Georgia_YK"
      ],
      "metadata": {
        "id": "xnT_RJV-ShUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this file is from df_nna_sample.xlsx with checking what language is used in the article on Google Sheet \n",
        "# how other languages than english is written in the random 10 articles \n",
        "df_sample_lang = pd.read_excel('df_nna_sample_wlang.xlsx') "
      ],
      "metadata": {
        "id": "_wHxYXpdK1cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_lang = df_sample_lang[['source', 'LANGUAGE']].dropna(subset=['LANGUAGE']).drop_duplicates()"
      ],
      "metadata": {
        "id": "nHz_rM9qK7cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_lang_article = pd.merge(df_sample_lang, df_nna, how=\"left\", on=\"source\")"
      ],
      "metadata": {
        "id": "4Dhma9CkLwdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_lang_article.to_excel('df_sample_lang_article_translation.xlsx')"
      ],
      "metadata": {
        "id": "fkA1IEGIL2DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_sample_lang_article[0:5000].to_excel('df_sample_lang_article_translation_file1.xlsx')\n",
        "df_sample_lang_article[5000:10000].to_excel('df_sample_lang_article_translation_file2.xlsx')\n",
        "df_sample_lang_article[10000:15000].to_excel('df_sample_lang_article_translation_file3.xlsx')\n",
        "df_sample_lang_article[15000:len(df_sample_lang_article)].to_excel('df_sample_lang_article_translation_file4.xlsx')"
      ],
      "metadata": {
        "id": "U15RGOy7M6Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translated version; files have been moved or deleted\n",
        "temp1 = pd.read_excel('translation/df_sample_lang_article_translation_file1.xlsx')\n",
        "temp2 = pd.read_excel('translation/df_sample_lang_article_translation_file2.xlsx')\n",
        "temp3 = pd.read_excel('translation/df_sample_lang_article_translation_file3.xlsx')\n",
        "temp4 = pd.read_excel('translation/df_sample_lang_article_translation_file4.xlsx')\n"
      ],
      "metadata": {
        "id": "q7Pthe4aYTaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_translation = pd.concat([temp1, temp2, temp3, temp4]) "
      ],
      "metadata": {
        "id": "lukXsQm9b8XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging those articles in other languages with those in English\n",
        "import numpy as np\n",
        "\n",
        "df_translation['article_text2'] = np.where(df_translation['article_text_translated'].isnull(), \n",
        "                                           df_translation['article_text'],\n",
        "                                           df_translation['article_text_translated'])\n"
      ],
      "metadata": {
        "id": "Y46J391NvFep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_translation.columns"
      ],
      "metadata": {
        "id": "mROyNgzVwJpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = df_translation.groupby(['source','LANGUAGE'])['article_text_translated'].count().reset_index()\n"
      ],
      "metadata": {
        "id": "_SYpCQx7dGV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[test['LANGUAGE']=='es']['article_text_translated'].sum()"
      ],
      "metadata": {
        "id": "5eOmVmDBo4Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[test['LANGUAGE']=='fil']['article_text_translated'].sum()"
      ],
      "metadata": {
        "id": "fV0RWEZ2qGwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[test['LANGUAGE']=='ko']['article_text_translated'].sum()"
      ],
      "metadata": {
        "id": "jv_3FB_-qTHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test[test['LANGUAGE']==\"es\"])"
      ],
      "metadata": {
        "id": "3RZij7pcoxts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample_lang_list = list(set(df_sample_lang.source.values.tolist()))\n",
        "df_source_list = list(set(df_nna.source.values.tolist()))\n"
      ],
      "metadata": {
        "id": "D-_p--WUsK3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_list_en = [i for i in df_source_list if i not in df_sample_lang_list]"
      ],
      "metadata": {
        "id": "_buW2WMJs2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_source_list_en))\n",
        "print(len(df_source_list))\n",
        "print(len(df_sample_lang_list))"
      ],
      "metadata": {
        "id": "fupxOeWqtT2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_list_en_only = pd.DataFrame(df_source_list_en, columns = [\"source\"])"
      ],
      "metadata": {
        "id": "Toul7wfDuFYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_list_en_only['en_source'] = 1"
      ],
      "metadata": {
        "id": "48U0DroBuKOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_list_en_only = pd.merge(df_source_list_en_only, df_nna, how=\"left\", on=\"source\")"
      ],
      "metadata": {
        "id": "iVSDrFcIuKHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_translation_done = df_translation[['source', 'website', 'article_url', 'article_title',\n",
        "       'article_author', 'date_published', 'article_text2']]\n"
      ],
      "metadata": {
        "id": "px9Lmnf3wVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge = pd.concat([df_source_list_en_only, df_translation_done])"
      ],
      "metadata": {
        "id": "mw4Ze04UuYZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_nna))\n",
        "print(len(df_source_merge))"
      ],
      "metadata": {
        "id": "Mhe12lllukQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge['article_final'] = np.where(df_source_merge['article_text2'].isnull(), df_source_merge['article_text'], df_source_merge['article_text2']) #CO: consider double-checking. I don't see anything wrong but I'm not familiar with np.where and this step seems important.)"
      ],
      "metadata": {
        "id": "bbu9Anz9wuGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge['article_final'].isnull().sum()"
      ],
      "metadata": {
        "id": "vdxCW6V-wNkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge_final = df_source_merge.drop(columns = ['article_text', 'article_text2'])"
      ],
      "metadata": {
        "id": "dCrLLRSFxCiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge_final.to_pickle('Georgia_data_v2_translation.pkl') # this is a first version of merge data with translation (deleted)"
      ],
      "metadata": {
        "id": "WG0HhPz6xKKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep for topic analysis"
      ],
      "metadata": {
        "id": "fbrosYrr-aUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Georgia/Topic Dictionaries"
      ],
      "metadata": {
        "id": "ffQuVIAKTdgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization if needed\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n"
      ],
      "metadata": {
        "id": "SdihvUuVapaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmas_num(text): \n",
        "\n",
        "    regex = re.compile('[^a-zA-Z0-9]')\n",
        "\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON') and (token.text != 'NA'): # spaCy has some issues by printing pronoun tagger\n",
        "            lemmas.append(regex.sub('', token.lemma_)) #CO: consider double-checking\n",
        "    \n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "yakNpLYL9r9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge_final['article_lemma_num'] = df_source_merge_final['article_final'].str.lower().astype(str).apply(get_lemmas_num) # lematization not require capitalization \n"
      ],
      "metadata": {
        "id": "ntrfoKZe-iW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_source_merge_final.to_pickle('Georgia_data_v2_translation_ner_lem.pkl') # commented this line to prevent from saving it"
      ],
      "metadata": {
        "id": "HSZvqDkHbXHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep for place analysis"
      ],
      "metadata": {
        "id": "JgTpKxDx-YMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Georgia/Locations Analysis"
      ],
      "metadata": {
        "id": "PFZz3K7xSlBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #CO: alread imported above\n",
        "\n",
        "df_source_merge_final = pd.read_pickle('/content/drive/MyDrive/Georgia/Web Scraping/Georgia_data_v2_translation_ner2_lem.pkl') #CO: file name differs from what I'm seeing in the folder\n"
      ],
      "metadata": {
        "id": "7WKF8OS4RkoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization if needed\n",
        "import spacy #CO: alread imported above\n",
        "from spacy import displacy\n",
        "\n",
        "sp = spacy.load('en_core_web_sm', disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n"
      ],
      "metadata": {
        "id": "Leu0aduT9jp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CO: this also seems to be in the script \"(2)_Georgia_place_analysis.ipynb\". Maybe you only need it once?\n",
        "\n",
        "def text_ner_place(text):\n",
        "  text = sp(text)\n",
        "  \n",
        "  ner_list = []\n",
        "  \n",
        "  for entity in text.ents:\n",
        "    if entity.label_ == \"FAC\":\n",
        "      ner_list.append(entity.text + \" (\" + entity.label_ + \")\")\n",
        "    if entity.label_ == \"GPE\":\n",
        "      ner_list.append(entity.text + \" (\" + entity.label_ + \")\")\n",
        "    if entity.label_ == \"LOC\":\n",
        "      ner_list.append(entity.text + \" (\" + entity.label_ + \")\")\n",
        "    else:\n",
        "      None\n",
        "  return str(set(ner_list))\n"
      ],
      "metadata": {
        "id": "r2S86oeO-Pqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_source_merge_final['ner_places'] = df_source_merge_final['article_final'].astype(str).apply(text_ner_place)"
      ],
      "metadata": {
        "id": "ydmHijaN-WNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_source_merge_final.to_pickle('Georgia_data_v2_translation_ner.pkl') # commented this line to prevent from saving it"
      ],
      "metadata": {
        "id": "lKyUNAawaALt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}